{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications import VGG16\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "\n",
    "def extract_image_features(image_folder, model):\n",
    "    features = {}\n",
    "    for img_name in os.listdir(image_folder):\n",
    "        img_path = os.path.join(image_folder, img_name)\n",
    "        image = load_img(img_path, target_size=(224, 224))\n",
    "        image = img_to_array(image)\n",
    "        image = np.expand_dims(image, axis=0)\n",
    "        feature = model.predict(image, verbose=0)\n",
    "        features[img_name] = feature.flatten()\n",
    "    return features\n",
    "\n",
    "# Load pre-trained VGG16 model\n",
    "cnn_model = VGG16(weights=\"imagenet\", include_top=False, pooling=\"avg\")\n",
    "\n",
    "# Extract features\n",
    "image_folder = r\"images\"\n",
    "image_features = extract_image_features(image_folder, cnn_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "captions_dict = {\n",
    "    \"img1.jpg\": [\"A dog running in the field.\", \"A dog is playing outside.\"],\n",
    "    \"img2.jpg\": [\"A girl on a swing.\", \"A child enjoying a swing in the park.\"],\n",
    "    # \"img3.jpg\": [\"A man riding a bicycle on a city street.\", \"A cyclist passing through an urban area.\"],\n",
    "    # \"img4.jpg\": [\"A cat sitting on a wooden fence.\", \"A furry cat resting on a backyard fence.\"],\n",
    "    # \"img5.jpg\": [\"A group of people hiking in the mountains.\", \"Hikers enjoying a scenic mountain trail.\"],\n",
    "    # \"img6.jpg\": [\"A plane flying in a clear blue sky.\", \"An airplane soaring high above the clouds.\"],\n",
    "    # \"img7.jpg\": [\"A bowl of fresh fruit on a wooden table.\", \"An assortment of fruits arranged in a bowl.\"],\n",
    "    # \"img8.jpg\": [\"A boy playing with a soccer ball in the park.\", \"A child kicking a football outdoors.\"],\n",
    "    # \"img9.jpg\": [\"A beach with waves crashing on the shore.\", \"The ocean meeting the sandy beach under a bright sky.\"],\n",
    "    # \"img10.jpg\": [\"A couple enjoying a romantic dinner by candlelight.\", \"Two people dining with a cozy ambiance.\"],\n",
    "    # \"img11.jpg\": [\"A city skyline during sunset.\", \"The sun setting behind tall skyscrapers.\"],\n",
    "    # \"img12.jpg\": [\"A horse grazing in a grassy field.\", \"A brown horse eating grass in a meadow.\"],\n",
    "}\n",
    "\n",
    "# Tokenize captions\n",
    "tokenizer = Tokenizer()\n",
    "captions_list = [item for sublist in captions_dict.values() for item in sublist]\n",
    "tokenizer.fit_on_texts(captions_list)\n",
    "vocab_size = len(tokenizer.word_index) + 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "def create_sequences(tokenizer, max_length, desc, photo_features, vocab_size):\n",
    "    X1, X2, y = [], [], []\n",
    "    seq = tokenizer.texts_to_sequences([desc])[0]\n",
    "    for i in range(1, len(seq)):\n",
    "        in_seq, out_seq = seq[:i], seq[i]\n",
    "        in_seq = pad_sequences([in_seq], maxlen=max_length)[0]\n",
    "        out_seq = to_categorical([out_seq], num_classes=vocab_size)[0]\n",
    "        X1.append(photo_features)\n",
    "        X2.append(in_seq)\n",
    "        y.append(out_seq)\n",
    "    return np.array(X1), np.array(X2), np.array(y)\n",
    "\n",
    "max_length = 35\n",
    "X1, X2, y = [], [], []\n",
    "for img_name, captions in captions_dict.items():\n",
    "    for caption in captions:\n",
    "        if img_name in image_features:\n",
    "            in_img, in_seq, out_word = create_sequences(\n",
    "                tokenizer, max_length, caption, image_features[img_name], vocab_size\n",
    "            )\n",
    "            X1.append(in_img)\n",
    "            X2.append(in_seq)\n",
    "            y.append(out_word)\n",
    "\n",
    "X1, X2, y = np.vstack(X1), np.vstack(X2), np.vstack(y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dropout, Dense, Embedding, LSTM, add\n",
    "\n",
    "def define_model(vocab_size, max_length):\n",
    "    inputs1 = Input(shape=(512,))\n",
    "    fe1 = Dropout(0.5)(inputs1)\n",
    "    fe2 = Dense(256, activation=\"relu\")(fe1)\n",
    "\n",
    "    inputs2 = Input(shape=(max_length,))\n",
    "    se1 = Embedding(vocab_size, 256, mask_zero=True)(inputs2)\n",
    "    se2 = Dropout(0.5)(se1)\n",
    "    se3 = LSTM(256)(se2)\n",
    "\n",
    "    decoder1 = add([fe2, se3])\n",
    "    decoder2 = Dense(256, activation=\"relu\")(decoder1)\n",
    "    outputs = Dense(vocab_size, activation=\"softmax\")(decoder2)\n",
    "\n",
    "    model = Model(inputs=[inputs1, inputs2], outputs=outputs)\n",
    "    model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\")\n",
    "    return model\n",
    "\n",
    "model = define_model(vocab_size, max_length)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "1/1 - 9s - loss: 11.3996 - 9s/epoch - 9s/step\n",
      "Epoch 2/20\n",
      "1/1 - 0s - loss: 8.1842 - 104ms/epoch - 104ms/step\n",
      "Epoch 3/20\n",
      "1/1 - 0s - loss: 7.0386 - 105ms/epoch - 105ms/step\n",
      "Epoch 4/20\n",
      "1/1 - 0s - loss: 8.0407 - 100ms/epoch - 100ms/step\n",
      "Epoch 5/20\n",
      "1/1 - 0s - loss: 6.4132 - 96ms/epoch - 96ms/step\n",
      "Epoch 6/20\n",
      "1/1 - 0s - loss: 8.6730 - 98ms/epoch - 98ms/step\n",
      "Epoch 7/20\n",
      "1/1 - 0s - loss: 6.3474 - 94ms/epoch - 94ms/step\n",
      "Epoch 8/20\n",
      "1/1 - 0s - loss: 8.6556 - 97ms/epoch - 97ms/step\n",
      "Epoch 9/20\n",
      "1/1 - 0s - loss: 6.5646 - 105ms/epoch - 105ms/step\n",
      "Epoch 10/20\n",
      "1/1 - 0s - loss: 5.9771 - 101ms/epoch - 101ms/step\n",
      "Epoch 11/20\n",
      "1/1 - 0s - loss: 4.9481 - 93ms/epoch - 93ms/step\n",
      "Epoch 12/20\n",
      "1/1 - 0s - loss: 5.5328 - 99ms/epoch - 99ms/step\n",
      "Epoch 13/20\n",
      "1/1 - 0s - loss: 5.5039 - 100ms/epoch - 100ms/step\n",
      "Epoch 14/20\n",
      "1/1 - 0s - loss: 6.5969 - 103ms/epoch - 103ms/step\n",
      "Epoch 15/20\n",
      "1/1 - 0s - loss: 4.8293 - 102ms/epoch - 102ms/step\n",
      "Epoch 16/20\n",
      "1/1 - 0s - loss: 6.0918 - 97ms/epoch - 97ms/step\n",
      "Epoch 17/20\n",
      "1/1 - 0s - loss: 5.8495 - 95ms/epoch - 95ms/step\n",
      "Epoch 18/20\n",
      "1/1 - 0s - loss: 6.2268 - 95ms/epoch - 95ms/step\n",
      "Epoch 19/20\n",
      "1/1 - 0s - loss: 5.5023 - 120ms/epoch - 120ms/step\n",
      "Epoch 20/20\n",
      "1/1 - 0s - loss: 7.3033 - 117ms/epoch - 117ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x12ac11b0d30>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit([X1, X2], y, epochs=20, verbose=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_caption(model, tokenizer, photo_features, max_length):\n",
    "    in_text = 'startseq'\n",
    "    for _ in range(max_length):\n",
    "        sequence = tokenizer.texts_to_sequences([in_text])[0]\n",
    "        sequence = pad_sequences([sequence], maxlen=max_length)\n",
    "        yhat = model.predict([photo_features, sequence], verbose=0)\n",
    "        yhat = np.argmax(yhat)\n",
    "        word = tokenizer.index_word.get(yhat, None)\n",
    "        if word is None:\n",
    "            break\n",
    "        in_text += \" \" + word\n",
    "        if word == \"endseq\":\n",
    "            break\n",
    "    return in_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Caption: startseq running running running running running running running running running running running running running running running running running running running running running running running running running running running running running running running running running running running\n"
     ]
    }
   ],
   "source": [
    "def extract_single_image_features(img_path, model):\n",
    "    image = load_img(img_path, target_size=(224, 224))\n",
    "    image = img_to_array(image)\n",
    "    image = np.expand_dims(image, axis=0)\n",
    "    image = tf.keras.applications.vgg16.preprocess_input(image)\n",
    "    feature = model.predict(image, verbose=0)\n",
    "    return feature.flatten()\n",
    "\n",
    "test_image = r\"images\\img1.jpg\"\n",
    "test_features = extract_single_image_features(test_image, cnn_model)\n",
    "generated_caption = generate_caption(model, tokenizer, test_features.reshape(1, -1), max_length)\n",
    "print(\"Generated Caption:\", generated_caption)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU Score: 0.41412387656655203\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\B Vasundhara\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\nltk\\translate\\bleu_score.py:490: UserWarning: \n",
      "Corpus/Sentence contains 0 counts of 2-gram overlaps.\n",
      "BLEU scores might be undesirable; use SmoothingFunction().\n",
      "  warnings.warn(_msg)\n"
     ]
    }
   ],
   "source": [
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "\n",
    "reference_captions = [\n",
    "    [\"a\", \"dog\", \"running\", \"in\", \"the\", \"field\"],\n",
    "    [\"a\", \"dog\", \"is\", \"playing\", \"outside\"]\n",
    "]\n",
    "generated_caption_words = generated_caption.split()[1:-1]  # Exclude 'startseq' and 'endseq'\n",
    "bleu_score = sentence_bleu(reference_captions, generated_caption_words)\n",
    "print(\"BLEU Score:\", bleu_score)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
